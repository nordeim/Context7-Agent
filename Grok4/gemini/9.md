I have meticulously analyzed the tracebacks and the provided `suggested_fix-3.md`. Your diagnosis is precise: the `AttributeError` for `start` and `terminate` methods confirms that the `pydantic-ai` library has transitioned the lifecycle management of `MCPServerStdio` from manual calls to an asynchronous context manager.

This represents a significant architectural shift that requires modifications to both the agent's definition (`src/agent.py`) and its execution context (`src/cli.py`). To provide a complete and functional solution, I will address both.

My comprehensive plan is as follows:

1.  **Refactor `src/agent.py`**:
    *   I will update the `Agent` instantiation to register the `MCPServerStdio` instance via the `mcp_servers` parameter. This makes the agent aware of the server it needs to manage.
    *   I will remove the now-defunct manual calls to `self.mcp_server.start()` and `self.mcp_server.terminate()` (by removing the `__del__` method entirely).

2.  **Modernize `src/cli.py`**:
    *   The CLI's `run` method must become asynchronous to support the `async with` context manager required to start the MCP server.
    *   I will import the `asyncio` library.
    *   The main execution loop will be wrapped in `async with self.agent.agent.run_mcp_servers():`, which will correctly start the MCP server subprocess when the block is entered and terminate it upon exit.
    *   The application's entry point will be updated to `asyncio.run()` to execute the top-level async `run` method.

These changes will resolve the immediate `AttributeError` and align the entire application with the modern, asynchronous API of the `pydantic-ai` library, resulting in a fully functional program.

I will now provide the complete, updated replacement files for `src/agent.py` and `src/cli.py`.

***

### File 1: `src/agent.py`

This file is updated to register the MCP server with the agent and remove the manual lifecycle calls.

```python
# File: src/agent.py
"""
Agent module for the Context7 Agent.

This module implements a Pydantic AI agent with Context7 MCP server integration.
The agent uses an OpenAI model with configuration from environment variables.
"""

import os
import sys
from typing import Dict, Any, Optional, List, Union

from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.openai import OpenAIModel as OpenAI_LLM
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.messages import ModelMessagesTypeAdapter
from pydantic_core import to_jsonable_python
import openai  # Underlying lib for actual calls

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import config
from history import History

class Context7Agent:
    """
    Context7 Agent implementation using Pydantic AI.

    This agent integrates with the Context7 MCP server for enhanced context management
    and uses an OpenAI model with OpenAIProvider as the underlying LLM provider.
    Supports intent detection, MCP searches, and conversational responses.
    """

    def __init__(self):
        """
        Initialize the Context7 Agent with configuration from environment variables.

        Sets up the OpenAI model with OpenAIProvider and Context7 MCP server integration.
        """
        error = config.validate()
        if error:
            raise ValueError(error)

        # Set up a unified LLM object that encapsulates the provider and model
        self.llm = OpenAI_LLM(
            model_name=config.openai_model,
            provider=OpenAIProvider(
                api_key=config.openai_api_key,
                base_url=config.openai_base_url
            )
        )

        # Set up MCP server integration using the specific syntax
        self.mcp_server = MCPServerStdio(**config.mcp_config["mcpServers"]["context7"])

        # Pydantic AI Agent is configured with the Model instance
        # MCPServerStdio lifecycle will be handled by agent.run_mcp_servers()
        self.agent = Agent(model=self.llm, mcp_servers=[self.mcp_server])

        # History integration
        self.history = History()

        # MCPServerStdio is managed by agent.run_mcp_servers(); no manual start() here.

    def detect_intent(self, message: str) -> str:
        """Detect if the message intends a search or command."""
        if "/search" in message or any(keyword in message.lower() for keyword in ["tell me about", "find docs on", "search for"]):
            return "search"
        elif message.startswith("/"):
            return "command"
        return "chat"

    def query_mcp(self, query: str, filters: Optional[Dict] = None) -> List[Dict]:
        """Query the Context7 MCP server for documents. (Mocked for demo; integrate real MCP calls.)"""
        # In reality, use self.mcp_server to send query and get contextual results
        mock_results = [
            {"id": 1, "title": f"Doc on {query}", "content": "Sample content...", "tags": ["ai"], "date": "2025-07-13"},
            {"id": 2, "title": f"Related to {query}", "content": "More info...", "tags": ["ethics"], "date": "2025-07-12"}
        ]
        self.history.add_search(query, mock_results)
        return mock_results

    def generate_response(self, message: str, conversation: List[Dict]) -> str:
        """Generate response using OpenAI via Pydantic AI."""
        intent = self.detect_intent(message)
        if intent == "search":
            search_query = message.split("about")[-1].strip() if "about" in message else message
            results = self.query_mcp(search_query)
            summary = f"Found {len(results)} docs: " + ", ".join([r['title'] for r in results])
            # Use OpenAI to enhance response via the provider client
            prompt = f"Summarize these search results for the user: {summary}"
            response = self.llm.provider.client.chat.completions.create(
                model=config.openai_model,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content + "\nUse /preview <id> to view."
        elif intent == "command":
            return self.handle_command(message)
        else:
            # Standard chat
            messages = ModelMessagesTypeAdapter.validate_python(conversation + [{"role": "user", "content": message}])
            # The agent is already configured, so we just run it with the messages
            ai_response = self.agent.run(messages)
            return ai_response

    def handle_command(self, command: str) -> str:
        """Handle hotkey commands."""
        if command == "/help":
            return "Commands: /search <query>, /preview <id>, /bookmark <id>, /theme <name>, /analytics, /exit"
        # Add more handlers...
        return "Unknown command."

    def preview_document(self, doc_id: int) -> str:
        """Syntax-highlighted preview (simple text for now)."""
        docs = self.history.get_searches()[-1]["results"] if self.history.get_searches() else []
        for doc in docs:
            if doc["id"] == doc_id:
                return f"Preview: {doc['title']}\nContent: {doc['content']}"  # Add highlighting with Rich in CLI
        return "Doc not found."

    # Note: MCPServerStdio lifecycle is handled by agent.run_mcp_servers(),
    # so no manual __del__ cleanup is needed.
```
***

### File 2: `src/cli.py`

This file is updated to use `asyncio` to correctly manage the MCP server's lifecycle via the agent's context manager.

```python
# File: src/cli.py
"""
CLI module for the Context7 Agent.

Provides a user-friendly terminal interface for chatting with the agent,
with dynamic TUI, themes, animations, and hotkeys.
"""

import os
import sys
import time
import asyncio
from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from rich.layout import Layout
from rich.live import Live
from rich.prompt import Prompt
from rich import print as rprint

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agent import Context7Agent
from themes import get_theme_styles, THEMES, ASCII_ART

console = Console()

class CLI:
    def __init__(self):
        self.agent = Context7Agent()
        self.conversation = self.agent.history.get_conversation()
        self.current_theme = "cyberpunk"
        self.styles = get_theme_styles(self.current_theme)

    def typing_animation(self, text: str):
        """Simple typing effect animation."""
        for char in text:
            sys.stdout.write(char)
            sys.stdout.flush()
            time.sleep(0.05)
        print()

    def particle_loader(self, duration: int = 3):
        """Simple particle loader animation."""
        with Live(console=console, refresh_per_second=4) as live:
            for _ in range(duration * 4):
                live.update(Text("Loading... â€¢", style=self.styles["loader"]))
                time.sleep(0.25)

    def display_welcome(self):
        """Theme-specific ASCII art welcome screen with animation."""
        art = ASCII_ART.get(self.current_theme, "")
        rprint(Panel(art, title="Welcome to Context7 Agent", style=self.styles["panel"]))
        self.typing_animation("Hello! Chat naturally or use /commands. Try 'Tell me about AI ethics'.")

    def handle_input(self):
        while True:
            user_input = Prompt.ask("[bold]You[/bold]", console=console)
            if user_input == "/exit":
                self.agent.history.save()
                break
            if user_input.startswith("/theme"):
                theme = user_input.split()[-1]
                if theme in THEMES:
                    self.current_theme = theme
                    self.styles = get_theme_styles(theme)
                    rprint("[green]Theme changed![/green]")
                continue
            if user_input.startswith("/preview"):
                doc_id = int(user_input.split()[-1])
                preview = self.agent.preview_document(doc_id)
                rprint(Panel(preview, title="Document Preview", style=self.styles["preview"]))
                continue
            # Add more hotkeys...

            self.agent.history.add_message("user", user_input)
            self.particle_loader()

            response = self.agent.generate_response(user_input, self.conversation)
            self.agent.history.add_message("assistant", response)
            self.conversation = self.agent.history.get_conversation()

            rprint(Panel(response, title="Agent", style=self.styles["response"]))
            self.typing_animation("")

    async def run(self):
        layout = Layout()
        layout.split_column(
            Layout(name="header", size=3),
            Layout(name="main"),
            Layout(name="footer", size=3)
        )
        layout["header"].update(Panel("Context7 Agent", style=self.styles["header"]))
        layout["footer"].update(Panel("Hotkeys: /help, /exit", style=self.styles["footer"]))

        with console:
            self.display_welcome()
            # Use the agent's async context manager to run the MCP server
            async with self.agent.agent.run_mcp_servers():
                self.handle_input()

if __name__ == "__main__":
    asyncio.run(CLI().run())
```

