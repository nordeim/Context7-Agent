I have conducted an exhaustive analysis of the traceback and the provided `suggested_fix_6.md`. Your diagnosis is exceptionally detailed and accurate. The error `Unable to extract tag using discriminator 'part_kind'` is the definitive proof that the `pydantic-ai` library's message schema requires yet another level of detail: a discriminator within each element of the `parts` list.

Your investigation correctly reveals that:
1.  The dictionary within the `parts` list is itself a discriminated union, keyed by `part_kind`.
2.  The `part_kind` value (`system-prompt`, `user-prompt`, `text`) and the payload's key (`content` vs. `text`) both depend on the message `role`.

The traceback confirms that the previously implemented helper function was still creating message parts without this required `part_kind` discriminator, leading directly to the validation failure.

The solution proposed in `suggested_fix_6.md` is surgically precise. By replacing the simple helper function with one that meticulously constructs the correct `part` dictionary based on the message role, it directly addresses the schema validation requirements.

My plan is to implement this exact fix. I will replace the inadequate `_to_model_message_dict` helper in `src/agent.py` with the new, correct version you have provided. This is a targeted change that will resolve the `ValidationError` and allow the application to proceed.

I will now provide the complete, updated replacement file for `src/agent.py`.

```python
# File: src/agent.py
"""
Agent module for the Context7 Agent.

This module implements a Pydantic AI agent with Context7 MCP server integration.
The agent uses an OpenAI model with configuration from environment variables.
"""

import os
import sys
from typing import Dict, Any, Optional, List, Union

from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.openai import OpenAIModel as OpenAI_LLM
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.messages import ModelMessagesTypeAdapter
from pydantic_core import to_jsonable_python
import openai

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import config
from history import History


def _to_model_message_dict(raw_msgs: list[dict]) -> list[dict]:
    """
    Convert each {'role','content'} into the full shape Pydantic-AI expects,
    with the correct part_kind discriminator on each part.
    """
    out: list[dict] = []
    for m in raw_msgs:
        role = m["role"]
        content = m["content"]
        if role == "system":
            kind = "request"
            part_kind = "system-prompt"
            payload = {"content": content}
        elif role == "user":
            kind = "request"
            part_kind = "user-prompt"
            payload = {"content": content}
        else:  # assistant or tool
            kind = "response"
            part_kind = "text"
            payload = {"text": content}

        part = {"part_kind": part_kind, **payload}
        out.append({"kind": kind, "role": role, "parts": [part]})
    return out


class Context7Agent:
    """
    Context7 Agent implementation using Pydantic AI.

    This agent integrates with the Context7 MCP server for enhanced context management
    and uses an OpenAI model with OpenAIProvider as the underlying LLM provider.
    Supports intent detection, MCP searches, and conversational responses.
    """
    def __init__(self):
        """
        Initialize the Context7 Agent with configuration from environment variables.

        Sets up the OpenAI model with OpenAIProvider and Context7 MCP server integration.
        """
        error = config.validate()
        if error:
            raise ValueError(error)

        self.llm = OpenAI_LLM(
            model_name=config.openai_model,
            provider=OpenAIProvider(
                api_key=config.openai_api_key,
                base_url=config.openai_base_url
            )
        )
        self.mcp_server = MCPServerStdio(**config.mcp_config["mcpServers"]["context7"])
        self.agent = Agent(model=self.llm, mcp_servers=[self.mcp_server])
        self.history = History()

    def detect_intent(self, message: str) -> str:
        """Detect if the message intends a search or command."""
        if "/search" in message or any(
            keyword in message.lower() for keyword in ["tell me about", "find docs on", "search for"]
        ):
            return "search"
        elif message.startswith("/"):
            return "command"
        return "chat"

    def query_mcp(self, query: str, filters: Optional[Dict] = None) -> List[Dict]:
        """Query the Context7 MCP server for documents. (Mocked for demo; integrate real MCP calls.)"""
        mock_results = [
            {"id": 1, "title": f"Doc on {query}", "content": "Sample content...", "tags": ["ai"], "date": "2025-07-13"},
            {"id": 2, "title": f"Related to {query}", "content": "More info...", "tags": ["ethics"], "date": "2025-07-12"}
        ]
        self.history.add_search(query, mock_results)
        return mock_results

    def generate_response(self, message: str, conversation: List[Dict]) -> str:
        """Generate response using OpenAI via Pydantic AI."""
        intent = self.detect_intent(message)
        if intent == "search":
            search_query = message.split("about")[-1].strip() if "about" in message else message
            results = self.query_mcp(search_query)
            summary = f"Found {len(results)} docs: " + ", ".join(r["title"] for r in results)
            prompt = f"Summarize these search results for the user: {summary}"
            response = self.llm.provider.client.chat.completions.create(
                model=config.openai_model,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content + "\nUse /preview <id> to view."
        elif intent == "command":
            return self.handle_command(message)
        else:
            raw_msgs = conversation + [{"role": "user", "content": message}]
            model_msgs = _to_model_message_dict(raw_msgs)
            messages = ModelMessagesTypeAdapter.validate_python(model_msgs)
            ai_response = self.agent.run(messages)
            return ai_response

    def handle_command(self, command: str) -> str:
        """Handle hotkey commands."""
        if command == "/help":
            return "Commands: /search <query>, /preview <id>, /bookmark <id>, /theme <name>, /analytics, /exit"
        # Add more handlers...
        return "Unknown command."

    def preview_document(self, doc_id: int) -> str:
        """Syntax-highlighted preview (simple text for now)."""
        docs = self.history.get_searches()[-1]["results"] if self.history.get_searches() else []
        for doc in docs:
            if doc["id"] == doc_id:
                return f"Preview: {doc['title']}\nContent: {doc['content']}"
        return "Doc not found."

    # Note: MCPServerStdio lifecycle is managed by agent.run_mcp_servers(); no manual cleanup needed.
```

