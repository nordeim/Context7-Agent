I have performed a deep and systematic analysis of the traceback. The error `TypeError: ModelRequest.__init__() got an unexpected keyword argument 'role'` is explicit and provides a clear path to the solution. It indicates that the constructor for the `pydantic_ai.messages.ModelRequest` class does not accept a `role` argument.

A detailed investigation into the `pydantic-ai` library's design, confirmed by reviewing its source code, reveals that the role of a message is not passed as an argument to the `ModelRequest` or `ModelResponse` constructors. Instead, the role is implicitly defined by the type of "Part" object (`UserPromptPart`, `SystemPromptPart`, etc.) contained within the `parts` list. This design choice uses Python's type system to convey information, which is a more robust pattern than relying on string arguments.

My previous implementation incorrectly passed the `role` as a keyword argument, leading directly to the `TypeError`.

My plan is to correct this by refactoring the `_to_pydantic_models` helper function in `src/agent.py`:

1.  I will remove the `role` keyword argument from the `ModelRequest` constructor calls for both "user" and "system" messages.
2.  For consistency and correctness, I will also remove the `role` keyword argument from the `ModelResponse` constructor call for "assistant" messages, as it follows the same design pattern.

This precise change aligns the code with the library's expected API, directly resolving the `TypeError` and allowing the application to proceed. I will now provide the complete, updated replacement file for `src/agent.py`.

```python
# File: src/agent.py
"""
Agent module for the Context7 Agent.

This module implements a Pydantic AI agent with Context7 MCP server integration.
The agent uses an OpenAI model with configuration from environment variables.
"""

import os
import sys
from typing import Dict, Any, Optional, List, Union

from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.openai import OpenAIModel as OpenAI_LLM
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.messages import ModelRequest, UserPromptPart, SystemPromptPart, TextPart, ModelResponse
import openai

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import config
from history import History


def _to_pydantic_models(raw_msgs: list[dict]) -> list[Union[ModelRequest, ModelResponse]]:
    """
    Convert a list of raw dictionaries into a list of Pydantic AI
    ModelRequest/ModelResponse objects.
    """
    models: list[Union[ModelRequest, ModelResponse]] = []
    for m in raw_msgs:
        role = m["role"]
        content = m["content"]
        if role == "user":
            models.append(ModelRequest(parts=[UserPromptPart(content=content)]))
        elif role == "system":
            models.append(ModelRequest(parts=[SystemPromptPart(content=content)]))
        elif role == "assistant":
            models.append(ModelResponse(parts=[TextPart(text=content)]))
    return models


class Context7Agent:
    """
    Context7 Agent implementation using Pydantic AI.

    This agent integrates with the Context7 MCP server for enhanced context management
    and uses an OpenAI model with OpenAIProvider as the underlying LLM provider.
    Supports intent detection, MCP searches, and conversational responses.
    """
    def __init__(self):
        """
        Initialize the Context7 Agent with configuration from environment variables.

        Sets up the OpenAI model with OpenAIProvider and Context7 MCP server integration.
        """
        error = config.validate()
        if error:
            raise ValueError(error)

        self.llm = OpenAI_LLM(
            model_name=config.openai_model,
            provider=OpenAIProvider(
                api_key=config.openai_api_key,
                base_url=config.openai_base_url
            )
        )
        self.mcp_server = MCPServerStdio(**config.mcp_config["mcpServers"]["context7"])
        self.agent = Agent(model=self.llm, mcp_servers=[self.mcp_server])
        self.history = History()

    def detect_intent(self, message: str) -> str:
        """Detect if the message intends a search or command."""
        if "/search" in message or any(
            keyword in message.lower() for keyword in ["tell me about", "find docs on", "search for"]
        ):
            return "search"
        elif message.startswith("/"):
            return "command"
        return "chat"

    def query_mcp(self, query: str, filters: Optional[Dict] = None) -> List[Dict]:
        """Query the Context7 MCP server for documents. (Mocked for demo; integrate real MCP calls.)"""
        mock_results = [
            {"id": 1, "title": f"Doc on {query}", "content": "Sample content...", "tags": ["ai"], "date": "2025-07-13"},
            {"id": 2, "title": f"Related to {query}", "content": "More info...", "tags": ["ethics"], "date": "2025-07-12"}
        ]
        self.history.add_search(query, mock_results)
        return mock_results

    async def generate_response(self, message: str, conversation: List[Dict]) -> str:
        """Generate response using OpenAI via Pydantic AI."""
        intent = self.detect_intent(message)
        if intent == "search":
            search_query = message.split("about")[-1].strip() if "about" in message else message
            results = self.query_mcp(search_query)
            summary = f"Found {len(results)} docs: " + ", ".join(r["title"] for r in results)
            prompt = f"Summarize these search results for the user: {summary}"
            response = self.llm.provider.client.chat.completions.create(
                model=config.openai_model,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content + "\nUse /preview <id> to view."
        elif intent == "command":
            return self.handle_command(message)
        else:
            # Convert the raw conversation history into Pydantic model instances
            # to ensure correct type information is passed to the agent.
            raw_msgs = conversation + [{"role": "user", "content": message}]
            pydantic_msgs = _to_pydantic_models(raw_msgs)
            ai_response = await self.agent.run(pydantic_msgs)
            return ai_response

    def handle_command(self, command: str) -> str:
        """Handle hotkey commands."""
        if command == "/help":
            return "Commands: /search <query>, /preview <id>, /bookmark <id>, /theme <name>, /analytics, /exit"
        # Add more handlers...
        return "Unknown command."

    def preview_document(self, doc_id: int) -> str:
        """Syntax-highlighted preview (simple text for now)."""
        docs = self.history.get_searches()[-1]["results"] if self.history.get_searches() else []
        for doc in docs:
            if doc["id"] == doc_id:
                return f"Preview: {doc['title']}\nContent: {doc['content']}"
        return "Doc not found."

    # Note: MCPServerStdio lifecycle is managed by agent.run_mcp_servers(); no manual cleanup needed.
```

### References

The resolution is based on a direct analysis of the traceback and an inspection of the `pydantic-ai` library's source code, which is a standard and necessary practice for debugging integration issues with third-party libraries.

1.  Pydantic-AI GitHub Repository. (2024). `messages.py` source file. This file defines the `ModelRequest` and `ModelResponse` classes and their constructors. Retrieved from https://github.com/pydantic/pydantic-ai/blob/main/src/pydantic_ai/messages.py

